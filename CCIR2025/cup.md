---
# TODO
title: CCIR 2025 法律知识检索与生成 大赛
# DONE
---

{% capture c1 %}

{% include conference_leftlist_CCIR2025.html %}

{% endcapture %}

{% capture c2 %}

<!-- TODO -->

# <i class="fas fa-feather-alt"></i> CCIR 2025 法律知识检索与生成 大赛

## 一、赛题背景

<p>近年来，检索增强型生成（Retrieval-Augmented Generation, RAG）技术在自然语言处理领域取得了显著进展。RAG技术通过结合信息检索和生成模型的优势，能够基于从可靠来源检索到的文档生成更准确、相关且符合上下文的回答。这种技术在多个领域表现出色，但在法律领域的应用仍处于探索阶段。法律咨询对话的复杂性在于问题的逐步展开、用户法律知识的缺乏以及对话中可能出现的突然话题转移，这些因素使得RAG系统在法律领域面临更大的挑战。</p>
<p>为了推动RAG技术在法律领域的应用和发展，我们构建了由法律专家标注的包括多轮法律对话的样本用于评测，并提供了包括法律条文、法律书籍、类案在内的法律知识库**作为检索参考，涵盖民事、刑事等多个法律细分领域。本次比赛将侧重于考察模型在会话知识检索和响应生成任务中的表现，特别是在多轮对话和复杂法律场景下的能力。</p>

## 二、赛题任务

本赛题构建了对话数据集，并提供丰富的法律条文库、知识库等作为检索参考，以评估预训练模型在法律领域不同难度任务上的检索与生成能力。

### 任务一：会话知识检索（Conversational Knowledge Retrieval）

模型需要根据多轮对话的上下文，从法律条文库中检索出与当前问题相关的法律条文。主要挑战在于多轮对话带来的指代消解、话题转移等问题。

评估指标包括 NDCG、Recall、MRR、Precision、F1 等。

### 任务二：响应生成（Response Generation）

模型需要结合对话历史和检索到的法律条文、法律知识，生成准确、连贯且符合法律要求的回答。主要挑战在于对复杂法律概念、条文和逻辑的理解和表述。

评估指标包括三方面：一是 ROUGE、BLEU、METEOR、BERTScore 等传统生成类评估指标；二是判断是否生成了相关的法律关键词；三是 LLM-as-a-Judge，通过多维思维链推理来多维度评估回答质量（如事实性、用户满意度、清晰度、逻辑连贯性和完整性）。

## 三、比赛时间线

- 赛题命制、数据标注：5 月 1 日-5 月 19 日
- 任务发布：5 月 20 日
- 初赛 A 榜评测：5 月 20 日-7 月 20 日
- 初赛 B 榜评测：7 月 21 日
- 成绩复现：7 月 22 日-7 月 31 日
- 决赛答辩：8 月 2 日
- 评测报告及颁奖：8 月 15-17 日

其中，A 榜包含约 1000 条评测样本（每个样本含有 3-5 轮对话），对话涉及日常法律咨询、法考知识点问答等基础场景；B 榜含约 100 条评测样本（每个样本含有 3-10 轮对话）。

## 四、评测数据下载链接

[https://github.com/CSHaitao/LexRAG](https://github.com/CSHaitao/LexRAG)

<!-- DONE -->

{% endcapture %}
{% include two-col.html leftcol=c1 rightcol=c2 left=3 right=9 %}
