---
# TODO
title: CCIR 2025 法律知识检索与生成 大赛
# DONE
---

{% capture c1 %}

{% include conference_leftlist_CCIR2025.html %}

{% endcapture %}

{% capture c2 %}

<!-- TODO -->

# <i class="fas fa-feather-alt"></i> CCIR 2025 法律知识检索与生成 大赛

## 一、赛题背景

近年来，检索增强型生成（Retrieval-Augmented Generation, RAG）技术在自然语言处理领域取得了显著进展。RAG 技术通过结合信息检索和生成模型的优势，能够基于从可靠来源检索到的文档生成更准确、相关且符合上下文的回答。这种技术在多个领域表现出色，但在法律领域的应用仍处于探索阶段。法律咨询对话的复杂性在于问题的逐步展开、用户法律知识的缺乏以及对话中可能出现的突然话题转移，这些因素使得 RAG 系统在法律领域面临更大的挑战。

为了推动 RAG 技术在法律领域的应用和发展，我们构建了由法律专家标注的包括多轮法律对话的样本用于评测，并提供了包括法律条文、法律书籍、类案在内的法律知识库\*\*作为检索参考，涵盖民事、刑事等多个法律细分领域。本次比赛将侧重于考察模型在会话知识检索和响应生成任务中的表现，特别是在多轮对话和复杂法律场景下的能力。

## 二、赛题任务

本赛题构建了对话数据集，并提供丰富的法律条文库、知识库等作为检索参考，以评估预训练模型在法律领域不同难度任务上的检索与生成能力。

### 任务一：会话知识检索（Conversational Knowledge Retrieval）

模型需要根据多轮对话的上下文，从法律条文库中检索出与当前问题相关的法律条文。主要挑战在于多轮对话带来的指代消解、话题转移等问题。

评估指标为 NDCG。

### 任务二：响应生成（Response Generation）

模型需要结合对话历史和检索到的法律条文、法律知识，生成准确、连贯且符合法律要求的回答。主要挑战在于对复杂法律概念、条文和逻辑的理解和表述。

评估指标包括 BERTScore（F1）和 keyword_accuracy 两种，加权得到该任务最终成绩。

## 三、比赛时间线

- 7 月 4 日：大赛启动，公布赛题，开放报名；
- 7 月 4 日-7 月 30 日：开放初赛 A 榜作品提交，此时间段内选手可提交作品，平台开放实时评测，3 次/队/天。同时平台会实时监测作弊行为；
- 7 月 30 日 24 点：截止大赛报名；
- 7 月 31 日：开放初赛 B 榜作品提交，此时间段内选手可提交作品，以选手当天提交的最后一次作品进行评测；
- 8 月 1 日-8 月 7 日：此时间段内工作人员会与 B 榜 TOP3 进行复现联系，如遇弃权、作弊等行为将顺延名次进行联系。被联系的队伍在此时间段需要按照主办方复现工作小组要求提交相关复现文件；选手准备答辩材料。
- 8 月 8 日：晋级队伍进行决赛答辩（具体待通知）；
- 8 月 14 日-8 月 18 日：获奖队伍出席 2025 年 CCIR 大会，进行颁奖典礼（具体待通知）。

## 四、参赛链接与提交方式

​ 参赛链接见：https://www.xir.cn/competitions/1116

​ 选手可在息壤平台进行报名，并获取相应的训练数据集、法律条文库以及评测数据，根据提交示例准备相应文件并在平台上提交，在 A 榜提交阶段平台将自动进行评分与排名。

<!-- DONE -->

{% endcapture %}
{% include two-col.html leftcol=c1 rightcol=c2 left=3 right=9 %}
